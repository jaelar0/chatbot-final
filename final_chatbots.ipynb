{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b58b294",
   "metadata": {},
   "source": [
    "## Chatbot with Attention LSTM RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49baa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "with open('./input/final_clean_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "START_TOKEN = 'STARTSEQ'\n",
    "END_TOKEN = 'ENDSEQ'\n",
    "\n",
    "input_texts = [item['input'] for item in data]\n",
    "target_texts = [f'{START_TOKEN} {item[\"output\"]} {END_TOKEN}' for item in data]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer_input = Tokenizer(filters='', lower=False)\n",
    "tokenizer_output = Tokenizer(filters='', lower=False)\n",
    "tokenizer_input.fit_on_texts(input_texts)\n",
    "tokenizer_output.fit_on_texts(target_texts)\n",
    "\n",
    "input_sequences = tokenizer_input.texts_to_sequences(input_texts)\n",
    "target_sequences = tokenizer_output.texts_to_sequences(target_texts)\n",
    "\n",
    "# Get optimal max lengths\n",
    "input_lengths = [len(seq) for seq in input_sequences]\n",
    "target_lengths = [len(seq) for seq in target_sequences]\n",
    "\n",
    "max_encoder_seq_length = int(np.percentile(input_lengths, 95))\n",
    "max_decoder_seq_length = int(np.percentile(target_lengths, 95)) + 1  # +1 to accommodate END_TOKEN\n",
    "\n",
    "# Pad sequences\n",
    "encoder_input_data = pad_sequences(input_sequences, maxlen=max_encoder_seq_length, padding='post')\n",
    "decoder_input_data = pad_sequences([seq[:-1] for seq in target_sequences], maxlen=max_decoder_seq_length - 1, padding='post')\n",
    "decoder_target_data = pad_sequences([seq[1:] for seq in target_sequences], maxlen=max_decoder_seq_length - 1, padding='post')\n",
    "decoder_target_data_one_hot = tf.keras.utils.to_categorical(decoder_target_data, num_classes=len(tokenizer_output.word_index) + 1).astype('float32')\n",
    "\n",
    "# Save updated metadata\n",
    "with open(\"chatbot_meta.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"max_encoder_seq_length\": max_encoder_seq_length,\n",
    "        \"max_decoder_seq_length\": max_decoder_seq_length,\n",
    "        \"latent_dim\": 128,\n",
    "        \"input_vocab_size\": len(tokenizer_input.word_index) + 1,\n",
    "        \"target_vocab_size\": len(tokenizer_output.word_index) + 1\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed5494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "latent_dim = 128\n",
    "input_vocab_size = len(tokenizer_input.word_index) + 1\n",
    "target_vocab_size = len(tokenizer_output.word_index) + 1\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,), name=\"encoder_inputs\")\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((input_vocab_size, embedding_dim))\n",
    "encoder_embedding = Embedding(input_vocab_size, embedding_dim, trainable=True, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"encoder_lstm\")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,), name=\"decoder_inputs\")\n",
    "decoder_embedding = Embedding(target_vocab_size, latent_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention\n",
    "attention_dot = Dot(axes=[2, 2], name=\"attention_dot\")([decoder_outputs, encoder_outputs])\n",
    "attention_softmax = Activation('softmax', name=\"attention_softmax\")(attention_dot)\n",
    "attention_context = Dot(axes=[2, 1], name=\"attention_context\")([attention_softmax, encoder_outputs])\n",
    "decoder_combined_context = Concatenate(axis=-1, name=\"concat_context\")([attention_context, decoder_outputs])\n",
    "\n",
    "# Output\n",
    "decoder_dense = Dense(target_vocab_size, activation='softmax', name=\"decoder_output\")\n",
    "decoder_outputs_final = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs_final)\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create sample weights to ignore padding tokens in loss\n",
    "sample_weights = (decoder_target_data != 0).astype('float32')\n",
    "\n",
    "# Train\n",
    "model.fit([encoder_input_data, decoder_input_data],\n",
    "          decoder_target_data_one_hot,\n",
    "          sample_weight=sample_weights,\n",
    "          batch_size=64,\n",
    "          epochs=800)\n",
    "\n",
    "# Save model\n",
    "model.save(\"chatbot_attention_model.keras\")\n",
    "\n",
    "# Save tokenizers\n",
    "with open(\"tokenizer_input.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer_input, f)\n",
    "with open(\"tokenizer_output.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer_output, f)\n",
    "\n",
    "# Save metadata\n",
    "with open(\"chatbot_meta.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"max_encoder_seq_length\": max_encoder_seq_length,\n",
    "        \"max_decoder_seq_length\": max_decoder_seq_length,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"input_vocab_size\": input_vocab_size,\n",
    "        \"target_vocab_size\": target_vocab_size\n",
    "    }, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Load tokenizers and metadata ---\n",
    "with open(\"tokenizer_input.pkl\", \"rb\") as f:\n",
    "    tokenizer_input = pickle.load(f)\n",
    "\n",
    "with open(\"tokenizer_output.pkl\", \"rb\") as f:\n",
    "    tokenizer_output = pickle.load(f)\n",
    "\n",
    "with open(\"chatbot_meta.json\", \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "max_encoder_seq_length = meta[\"max_encoder_seq_length\"]\n",
    "max_decoder_seq_length = meta[\"max_decoder_seq_length\"]\n",
    "latent_dim = meta[\"latent_dim\"]\n",
    "\n",
    "START_TOKEN = 'STARTSEQ'\n",
    "END_TOKEN = 'ENDSEQ'\n",
    "\n",
    "reverse_target_word_index = {v: k for k, v in tokenizer_output.word_index.items()}\n",
    "target_word_index = tokenizer_output.word_index\n",
    "\n",
    "# --- Load trained model ---\n",
    "model = load_model(\"chatbot_attention_model.keras\")\n",
    "\n",
    "# --- Rebuild encoder model ---\n",
    "encoder_inputs = model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.get_layer(\"encoder_lstm\").output\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h_enc, state_c_enc])\n",
    "\n",
    "# --- Rebuild decoder model with attention ---\n",
    "decoder_inputs = model.input[1]\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"decoder_state_input_c\")\n",
    "decoder_hidden_state_input = Input(shape=(max_encoder_seq_length, latent_dim), name=\"decoder_hidden_state_input\")\n",
    "\n",
    "decoder_embedding = model.get_layer(\"decoder_embedding\")(decoder_inputs)\n",
    "decoder_lstm = model.get_layer(\"decoder_lstm\")\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")\n",
    "\n",
    "attention_dot = Dot(axes=[2, 2], name=\"attention_dot\")([decoder_outputs, decoder_hidden_state_input])\n",
    "attention_softmax = Activation('softmax', name=\"attention_softmax\")(attention_dot)\n",
    "attention_context = Dot(axes=[2, 1], name=\"attention_context\")([attention_softmax, decoder_hidden_state_input])\n",
    "decoder_combined_context = Concatenate(axis=-1, name=\"concat_context\")([attention_context, decoder_outputs])\n",
    "\n",
    "decoder_dense = model.get_layer(\"decoder_output\")\n",
    "decoder_outputs_final = decoder_dense(decoder_combined_context)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs_final, state_h_dec, state_c_dec]\n",
    ")\n",
    "\n",
    "# --- Decode function ---\n",
    "def decode_sequence(input_text):\n",
    "    input_seq = tokenizer_input.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_encoder_seq_length, padding='post')\n",
    "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.array([[target_word_index[START_TOKEN]]])\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    for _ in range(max_decoder_seq_length):\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq, encoder_outputs, state_h, state_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_target_word_index.get(sampled_token_index, '')\n",
    "\n",
    "        if sampled_word == END_TOKEN:\n",
    "            break\n",
    "\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        target_seq = np.array([[sampled_token_index]])\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "# --- Jupyter chatbot interface ---\n",
    "greeting_inputs = {\"hi\", \"hello\", \"hey\", \"hi there\", \"hey there\"}\n",
    "greeting_response = \"Hello! I'm your financial assistant. Ask me anything about finance terms.\"\n",
    "exit_commands = {\"bye\", \"exit\", \"quit\", \"goodbye\"}\n",
    "\n",
    "input_box = widgets.Text(placeholder=\"Type your question here...\", layout=widgets.Layout(width='90%'))\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def handle_message(widget):\n",
    "    user_input = widget.value.strip()\n",
    "    if not user_input:\n",
    "        return\n",
    "    input_box.value = \"\"\n",
    "\n",
    "    with output_area:\n",
    "        print(f\"You: {user_input}\")\n",
    "\n",
    "        if user_input.lower() in exit_commands:\n",
    "            print(\"FinancialBot: Goodbye! Stay financially smart!\")\n",
    "            input_box.disabled = True\n",
    "            return\n",
    "\n",
    "        if user_input.lower() in greeting_inputs:\n",
    "            print(f\"FinancialBot: {greeting_response}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            response = decode_sequence(user_input)\n",
    "            if response:\n",
    "                print(\"FinancialBot:\", response)\n",
    "            else:\n",
    "                print(\"FinancialBot: I'm not sure I understood that. Can you rephrase?\")\n",
    "        except Exception as e:\n",
    "            print(\"FinancialBot: Sorry, something went wrong.\")\n",
    "            print(f\"[Debug]: {e}\")\n",
    "\n",
    "input_box.on_submit(handle_message)\n",
    "\n",
    "display(Markdown(\"### FinancialBot Chat Interface\"))\n",
    "display(input_box)\n",
    "display(output_area)\n",
    "\n",
    "with output_area:\n",
    "    print(\"FinancialBot: Hello! Ask me any finance question, or type 'exit' to quit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b70ed8",
   "metadata": {},
   "source": [
    "## Chatbot using Transformer Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91df6f",
   "metadata": {},
   "source": [
    "## Using google's flan-t5-base model\n",
    "- Train on GPU - special conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb727b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# make sure pyTorch is configured so we can use apple metal\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Detect Apple MPS GPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load data\n",
    "with open(\"./input/final_clean_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Clean and load dataset\n",
    "clean_data = [ex for ex in data if ex[\"output\"].strip()]\n",
    "dataset = Dataset.from_list(clean_data)\n",
    "\n",
    "# Load tokenizer/model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(example):\n",
    "    input_text = \"question: \" + example[\"input\"]\n",
    "    target_text = example[\"output\"]\n",
    "    model_inputs = tokenizer(\n",
    "        input_text, max_length=64, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target = tokenizer(\n",
    "            target_text, max_length=64, padding=\"max_length\", truncation=True\n",
    "        )\n",
    "    labels = target[\"input_ids\"]\n",
    "    labels = [(label if label != tokenizer.pad_token_id else -100) for label in labels]\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, remove_columns=[\"input\", \"output\"])\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Training args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan_t5_mps_model\",\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=False  # AMP not supported on MPS\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save\n",
    "model.save_pretrained(\"./flan_t5_mps_model\")\n",
    "tokenizer.save_pretrained(\"./flan_t5_mps_model\")\n",
    "print(\"Model and tokenizer saved to ./flan_t5_mps_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010adb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load model & tokenizer\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_path = \"./flan_t5_mps_model\"\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path, local_files_only=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "\n",
    "# Inference function\n",
    "def generate_answer(question):\n",
    "    prompt = \"question: \" + question\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=64,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# UI\n",
    "input_box = widgets.Text(placeholder=\"Ask a finance question...\", layout=widgets.Layout(width='90%'))\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def handle_submit(widget):\n",
    "    query = widget.value.strip()\n",
    "    input_box.value = \"\"\n",
    "    if not query:\n",
    "        return\n",
    "    with output_area:\n",
    "        print(f\"You: {query}\")\n",
    "        try:\n",
    "            response = generate_answer(query)\n",
    "            print(\"Bot:\", response)\n",
    "        except Exception as e:\n",
    "            print(\"Bot: Something went wrong.\")\n",
    "            print(f\"[Error]: {e}\")\n",
    "\n",
    "input_box.on_submit(handle_submit)\n",
    "display(Markdown(\"### ðŸ’¬ FinancialBot (FLAN-T5 MPS)\"))\n",
    "display(input_box)\n",
    "display(output_area)\n",
    "\n",
    "with output_area:\n",
    "    print(\"Bot: Ready! Ask me anything about finance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14f70e",
   "metadata": {},
   "source": [
    "## ROGUE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de7e682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41238167885565946"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"./flan_t5_mps_model\"\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_path, local_files_only=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "\n",
    "# Load data\n",
    "with open(\"./input/final_clean_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Evaluate on only 20 samples\n",
    "subset = data[:20]\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for example in subset:\n",
    "    input_text = \"question: \" + example[\"input\"]\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=64).input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, max_length=64, do_sample=False)\n",
    "    prediction = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(example[\"output\"])\n",
    "\n",
    "# Basic ROUGE-L\n",
    "def lcs(X, Y):\n",
    "    m, n = len(X), len(Y)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if X[i] == Y[j]:\n",
    "                dp[i + 1][j + 1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i + 1][j + 1] = max(dp[i][j + 1], dp[i + 1][j])\n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(pred, ref):\n",
    "    pred_tokens, ref_tokens = pred.split(), ref.split()\n",
    "    lcs_len = lcs(pred_tokens, ref_tokens)\n",
    "    if lcs_len == 0:\n",
    "        return 0.0\n",
    "    prec = lcs_len / len(pred_tokens)\n",
    "    rec = lcs_len / len(ref_tokens)\n",
    "    return (2 * prec * rec) / (prec + rec) if (prec + rec) else 0.0\n",
    "\n",
    "rouge_scores = [rouge_l(p, r) for p, r in zip(predictions, references)]\n",
    "avg_rouge_l = sum(rouge_scores) / len(rouge_scores)\n",
    "avg_rouge_l\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
